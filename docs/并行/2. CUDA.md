# CUDA

CUDA中的计算模型采取CPU与GPU相结合的主机端—客户端架构，其中CPU作为主机端，主要负责资源调度、配置线程网格参数以及调用在GPU端运行的核函数，GPU作为客户端，读取线程网格参数后，在每个线程上执行对应的核函数，从而实现大规模并行计算。

逻辑上看，线程是GPU计算的基础组件，多个线程组成线程块，多个线程块组成线程网格，而从硬件的角度看，以 Nvidia Turing 架构为例，CUDA 核心是 GPU 计算的基础组件，多个CUDA核心组成流式多处理器（stream multiprocessor，SM），实际执行时，一个线程块在一个 SM 上执行，线程块中的线程则在该 SM 中的 CUDA 核心上执行，SM 接收到线程块输入时，会将其进一步划分为线程束，而线程束的执行时间取决于其中耗时最久的线程，因此，为最大化 SM 的执行效率，需要正确选择线程块的尺寸。
